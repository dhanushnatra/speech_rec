{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "mel_transform = T.MelSpectrogram(\n",
    "    sample_rate=16000,\n",
    "    n_fft=1024,\n",
    "    win_length=1024,\n",
    "    hop_length=256,\n",
    "    n_mels=80,\n",
    "    f_min=0,\n",
    "    f_max=8000,\n",
    "    power=1.0,\n",
    "    normalized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AtoZ={\n",
    "    'a': 0,\n",
    "    'b': 1,\n",
    "    'c': 2,\n",
    "    'd': 3,\n",
    "    'e': 4,\n",
    "    'f': 5,\n",
    "    'g': 6,\n",
    "    'h': 7,\n",
    "    'i': 8,\n",
    "    'j': 9,\n",
    "    'k': 10,\n",
    "    'l': 11,\n",
    "    'm': 12,\n",
    "    'n': 13,\n",
    "    'o': 14,\n",
    "    'p': 15,\n",
    "    'q': 16,\n",
    "    'r': 17,\n",
    "    's': 18,\n",
    "    't': 19,\n",
    "    'u': 20,\n",
    "    'v': 21,\n",
    "    'w': 22,\n",
    "    'x': 23,\n",
    "    'y': 24,\n",
    "    'z': 25,\n",
    "    \" \": 26,\n",
    "}\n",
    "def get_label_from_text(text:str)->list:\n",
    "    \"\"\"\n",
    "    Convert text to a list of integers.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    return [AtoZ[char] for char in text if char in AtoZ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 4, 11, 11, 14, 26, 22, 14, 17, 11, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_from_text(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_paths, labels):\n",
    "        self.audio_paths = audio_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def load_audio(file_path):\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "            return waveform, sample_rate\n",
    "        waveform, _= load_audio(self.audio_paths[idx])\n",
    "        mel = mel_transform(waveform)\n",
    "        label = torch.tensor(get_label_from_text(self.labels[idx]), dtype=torch.long)\n",
    "        return mel.squeeze(0), label\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv(\"dataset/en/validated.tsv\", sep=\"\\t\")\n",
    "df[\"path\"]= \"dataset/en/clips/\"+df[\"path\"]\n",
    "df[\"sentence\"]= df[\"sentence\"].astype(str)\n",
    "X=df[\"path\"].tolist()\n",
    "Y=df[\"sentence\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.2611e-05, 5.1856e-05, 1.5952e-04,  ..., 5.2414e-03, 2.6059e-03,\n",
       "          1.1178e-02],\n",
       "         [1.4391e-05, 3.4026e-05, 3.6697e-05,  ..., 1.1884e-02, 9.2271e-03,\n",
       "          1.0617e-02],\n",
       "         [4.8470e-06, 6.8994e-06, 6.1866e-06,  ..., 1.8028e-02, 1.4053e-02,\n",
       "          1.9148e-02],\n",
       "         ...,\n",
       "         [1.8270e-07, 1.9154e-07, 2.7454e-07,  ..., 5.7450e-07, 1.3754e-04,\n",
       "          7.9204e-04],\n",
       "         [2.1495e-07, 2.6207e-07, 2.8719e-07,  ..., 4.7664e-07, 1.3295e-04,\n",
       "          7.6554e-04],\n",
       "         [1.5080e-07, 2.1374e-07, 2.3231e-07,  ..., 5.4438e-07, 1.3185e-04,\n",
       "          7.5922e-04]]),\n",
       " tensor([19,  7,  4, 26, 14, 20, 19,  4, 17, 26, 17,  8, 12, 26,  7,  0, 18, 26,\n",
       "         20, 13,  3,  4, 17,  6, 14, 13,  4, 26, 18, 14, 12,  4, 26,  4, 17, 14,\n",
       "         18,  8, 14, 13, 26,  3, 20,  4, 26, 19, 14, 26, 18, 20,  1, 18,  4, 16,\n",
       "         20,  4, 13, 19, 26,  8, 12, 15,  0,  2, 19, 18]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = AudioDataset(audio_paths=X, labels=Y)\n",
    "train_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
